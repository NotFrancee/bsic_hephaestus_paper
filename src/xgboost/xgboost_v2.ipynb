{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "\n",
    "from tune_model import tune #function to tune hyperparameters\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, precision_score, recall_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "data = pd.read_csv('../../data/definitive_dataset.csv')\n",
    "\n",
    "# convert date columns to date with no time\n",
    "data['date'] = pd.to_datetime(data['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the features as pickle\n",
    "with open('../../data/selected_features.pkl', 'rb') as f:\n",
    "    features = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with missing target value\n",
    "data.dropna(subset=['target'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss_function(y_true, y_pred):\n",
    "    print(y_pred)\n",
    "    print(y_true)\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define starting dates for the periods\n",
    "starting_dates = ['2008-01-01', '2010-01-01', '2012-01-01', '2014-01-01', '2016-01-01', '2018-01-01']\n",
    "\n",
    "# define the model\n",
    "model = XGBClassifier()\n",
    "\n",
    "# define the hyperparameters to tune\n",
    "hyperparameters = {\n",
    "    'n_estimators': [1000],\n",
    "    'max_depth': [11],\n",
    "    'min_child_weight': [1],\n",
    "    'gamma': [0],\n",
    "    'eta': [0.3],\n",
    "    'alpha': [1.5],\n",
    "}\n",
    "# MISSING: min_child_weight\n",
    "\n",
    "\n",
    "# define a multi metric to use in the tuning\n",
    "scoring = {\n",
    "    'Accuracy': 'accuracy',\n",
    "    'AUC': 'roc_auc',\n",
    "    'F1': 'f1',\n",
    "    'Precision': 'precision',\n",
    "    'Recall': 'recall'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for the period 2008-01-01 to 2009-10-02...\n",
      "Number of features: 33\n",
      "Length of training data: 109413, original length: 272473, removed 163060 outliers.\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "\n",
      "Best Score (AUC): \u001b[1m0.9244303395201674\u001b[0m\n",
      "Accuracy: 0.8468006503787948\n",
      "F1: 0.8554276846541559\n",
      "Precision: 0.8548082817019791\n",
      "Recall: 0.8560626985223035\n",
      "\n",
      "Best Hyperparameters:\n",
      "alpha: 1.5\n",
      "eta: 0.3\n",
      "gamma: 0\n",
      "max_depth: 11\n",
      "min_child_weight: 1\n",
      "n_estimators: 1000\n",
      "scale_pos_weight: 0.8887757215854164\n",
      "\n",
      "-------------------------------------------------- \n",
      "\n",
      "Training model for the period 2010-01-01 to 2011-10-02...\n",
      "Number of features: 34\n",
      "Length of training data: 115406, original length: 271016, removed 155610 outliers.\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "\n",
      "Best Score (AUC): \u001b[1m0.9103690378634695\u001b[0m\n",
      "Accuracy: 0.8347312964663882\n",
      "F1: 0.8609060731183011\n",
      "Precision: 0.8540695463312478\n",
      "Recall: 0.867863454068371\n",
      "\n",
      "Best Hyperparameters:\n",
      "alpha: 1.5\n",
      "eta: 0.3\n",
      "gamma: 0\n",
      "max_depth: 11\n",
      "min_child_weight: 1\n",
      "n_estimators: 1000\n",
      "scale_pos_weight: 0.6968226662549807\n",
      "\n",
      "-------------------------------------------------- \n",
      "\n",
      "Training model for the period 2012-01-01 to 2013-10-02...\n",
      "Number of features: 34\n",
      "Length of training data: 111353, original length: 268808, removed 157455 outliers.\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "\n",
      "Best Score (AUC): \u001b[1m0.9079440675583245\u001b[0m\n",
      "Accuracy: 0.8401300394775055\n",
      "F1: 0.8772158284006828\n",
      "Precision: 0.87038526345412\n",
      "Recall: 0.8841554974695511\n",
      "\n",
      "Best Hyperparameters:\n",
      "alpha: 1.5\n",
      "eta: 0.3\n",
      "gamma: 0\n",
      "max_depth: 11\n",
      "min_child_weight: 1\n",
      "n_estimators: 1000\n",
      "scale_pos_weight: 0.5482036594182749\n",
      "\n",
      "-------------------------------------------------- \n",
      "\n",
      "Training model for the period 2014-01-01 to 2015-10-02...\n",
      "Number of features: 35\n",
      "Length of training data: 135369, original length: 267018, removed 131649 outliers.\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "\n",
      "Best Score (AUC): \u001b[1m0.9067336195418816\u001b[0m\n",
      "Accuracy: 0.8295399860098406\n",
      "F1: 0.8464353615887654\n",
      "Precision: 0.8399912991385059\n",
      "Recall: 0.8529829926498202\n",
      "\n",
      "Best Hyperparameters:\n",
      "alpha: 1.5\n",
      "eta: 0.3\n",
      "gamma: 0\n",
      "max_depth: 11\n",
      "min_child_weight: 1\n",
      "n_estimators: 1000\n",
      "scale_pos_weight: 0.8156687590535973\n",
      "\n",
      "-------------------------------------------------- \n",
      "\n",
      "Training model for the period 2016-01-01 to 2017-10-02...\n",
      "Number of features: 35\n",
      "Length of training data: 129129, original length: 252712, removed 123583 outliers.\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "\n",
      "Best Score (AUC): \u001b[1m0.9102238477208344\u001b[0m\n",
      "Accuracy: 0.8472535215430207\n",
      "F1: 0.8863117112901853\n",
      "Precision: 0.8779423541792666\n",
      "Recall: 0.8948428098780947\n",
      "\n",
      "Best Hyperparameters:\n",
      "alpha: 1.5\n",
      "eta: 0.3\n",
      "gamma: 0\n",
      "max_depth: 11\n",
      "min_child_weight: 1\n",
      "n_estimators: 1000\n",
      "scale_pos_weight: 0.5029155367264516\n",
      "\n",
      "-------------------------------------------------- \n",
      "\n",
      "Training model for the period 2018-01-01 to 2019-10-02...\n",
      "Number of features: 36\n",
      "Length of training data: 95994, original length: 245471, removed 149477 outliers.\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "\n",
      "Best Score (AUC): \u001b[1m0.9202713038640872\u001b[0m\n",
      "Accuracy: 0.8436985644936141\n",
      "F1: 0.8598225818480074\n",
      "Precision: 0.8529593854846116\n",
      "Recall: 0.8668060578661845\n",
      "\n",
      "Best Hyperparameters:\n",
      "alpha: 1.5\n",
      "eta: 0.3\n",
      "gamma: 0\n",
      "max_depth: 11\n",
      "min_child_weight: 1\n",
      "n_estimators: 1000\n",
      "scale_pos_weight: 0.8082052441229657\n",
      "\n",
      "-------------------------------------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tune a model for each starting date\n",
    "for start_date in starting_dates:\n",
    "\n",
    "    # convert the start date to datetime\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "    \n",
    "    # define the end date\n",
    "    end_date = start_date + pd.DateOffset(years=2) - pd.DateOffset(days=91)\n",
    "\n",
    "    # convert the dates to strings\n",
    "    start_date_str = str(start_date.date())\n",
    "    end_date_str = str(end_date.date()) \n",
    "\n",
    "    print(f'Training model for the period {start_date_str} to {end_date_str}...')\n",
    "    \n",
    "    # subset the data to the period of interest\n",
    "    training_data = data[(data['date'] >= start_date) & (data['date'] <= end_date)]\n",
    "\n",
    "    # find the selected features for the period of interest\n",
    "    training_features = features[start_date]\n",
    "    print(f'Number of features: {len(training_features)}')\n",
    "\n",
    "    initial_length = len(training_data)\n",
    "\n",
    "    #Â remove outliers from features\n",
    "    for feature in training_features:\n",
    "        q1 = training_data[feature].quantile(0.25)\n",
    "        q3 = training_data[feature].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - 3.5*iqr\n",
    "        upper_bound = q3 + 3.5*iqr\n",
    "        training_data = training_data[(training_data[feature] >= lower_bound) & (training_data[feature] <= upper_bound)]\n",
    "    print(f'Length of training data: {len(training_data)}, original length: {initial_length}, removed {initial_length - len(training_data)} outliers.')\n",
    "\n",
    "    X = training_data[training_features].copy()\n",
    "    y = training_data['target'].copy()\n",
    "\n",
    "\n",
    "    # find the scale_pos_weight\n",
    "    scale_pos_weight = y.value_counts()[0] / y.value_counts()[1]\n",
    "\n",
    "    # add the scale_pos_weight to the hyperparameters\n",
    "    hyperparameters['scale_pos_weight'] = [scale_pos_weight]\n",
    "\n",
    "    # tune the model\n",
    "    best_params, best_model = tune(X=X, y=y, space=hyperparameters,\n",
    "                                   model=model, search_type='grid', \n",
    "                                   n_splits=2, n_repeats=1, scoring=scoring,\n",
    "                                   refit='AUC')\n",
    "\n",
    "    # save the best model\n",
    "    with open(f'../../models/xgb_{start_date_str}.pkl', 'wb') as f:\n",
    "        pkl.dump(best_model, f)\n",
    "    \n",
    "    print('-'*50, '\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
