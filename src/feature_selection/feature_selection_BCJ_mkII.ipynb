{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_stock_data(data, start_date, end_date, verbose=False):\n",
    "    \"\"\"\n",
    "    Subsets the given dataframe based on a specified date range.\n",
    "\n",
    "    Args:\n",
    "        data (pandas.DataFrame): The dataframe containing the stock data.\n",
    "        start_date (str or pandas.Timestamp): The start date of the desired date range.\n",
    "        end_date (str or pandas.Timestamp): The end date of the desired date range.\n",
    "        verbose (bool, optional): If True, prints a success message. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The subset of the dataframe based on the specified date range.\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If the dataframe does not contain a 'Date' column.\n",
    "\n",
    "    \"\"\"\n",
    "    # Check if 'Date' column exists in the dataframe\n",
    "    if 'date' not in data.columns:\n",
    "        raise ValueError(\"DataFrame does not contain a 'Date' column.\")\n",
    "     \n",
    "    # Convert date columns to datetime if they are not already datetime objects\n",
    "    if not isinstance(data['date'], pd.DatetimeIndex):\n",
    "        data['date'] = pd.to_datetime(data['date'])\n",
    "    \n",
    "    if not isinstance(start_date, pd.Timestamp):\n",
    "        start_date = pd.to_datetime(start_date)\n",
    "\n",
    "    if not isinstance(end_date, pd.Timestamp):\n",
    "        end_date = pd.to_datetime(end_date)\n",
    "\n",
    "    # Subset the dataframe based on date range\n",
    "    subset = data[(data['date'] >= start_date) & (data['date'] <=end_date)]\n",
    "    if verbose:\n",
    "        print(f'Successfully subsetted data from {start_date} to {end_date}.')\n",
    "    return subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_numerical_columns(data, verbose=False):\n",
    "    \"\"\"\n",
    "    Remove non-numerical columns from a dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    - data: pandas DataFrame\n",
    "        The input dataframe from which non-numerical columns will be removed.\n",
    "    - verbose: bool, optional\n",
    "        If True, print a message with the deleted columns. Default is False.\n",
    "\n",
    "    Returns:\n",
    "    - pandas DataFrame\n",
    "        The dataframe with non-numerical columns removed.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check first 10 rows for numerical columns\n",
    "    first_10_rows = data.head(10)\n",
    "    non_numerical_columns = []\n",
    "\n",
    "    # Iterate through columns\n",
    "    for column in data.columns:\n",
    "        # Check if the column contains numerical data\n",
    "        if pd.api.types.is_numeric_dtype(first_10_rows[column]):\n",
    "            continue\n",
    "        else:\n",
    "            non_numerical_columns.append(column)\n",
    "\n",
    "    # Remove non-numerical columns from the dataframe\n",
    "    data = data.copy()\n",
    "    data.drop(columns=non_numerical_columns, inplace=True)\n",
    "\n",
    "    # drop date column if it exists\n",
    "    if 'date' in data.columns:\n",
    "        data.drop(columns=['date'], inplace=True)\n",
    "\n",
    "    # Print message with deleted columns\n",
    "    if verbose:\n",
    "        if non_numerical_columns:\n",
    "            print(\"Successfully removed columns with non-numerical values:\", non_numerical_columns)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data, interpolate = False, start_date=None, end_date=None, verbose=False):\n",
    "    \"\"\"\n",
    "    Preprocesses the input data by performing the following steps:\n",
    "    1. Subset the data based on the specified start and end dates.\n",
    "    2. Remove non-numerical columns from the subsetted data.\n",
    "    3. Scale the numerical data using StandardScaler.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The input data to be preprocessed.\n",
    "        start_date (str, optional): The start date for subsetting the data. Defaults to None.\n",
    "        end_date (str, optional): The end date for subsetting the data. Defaults to None.\n",
    "        verbose (bool, optional): Whether to print verbose output. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The preprocessed and scaled data.\n",
    "    \"\"\"\n",
    "    if type(data) != pd.DataFrame:\n",
    "        raise Exception('data must be a pandas dataframe')\n",
    "    \n",
    "    subset_data = subset_stock_data(data, start_date, end_date, verbose=verbose)\n",
    "    subset_numerical_data = remove_non_numerical_columns(subset_data, verbose=verbose)\n",
    "\n",
    "    if interpolate:\n",
    "        subset_numerical_data = subset_numerical_data.interpolate(limit_direction='forward')\n",
    "\n",
    "    if 'permno' in subset_numerical_data.columns:\n",
    "        subset_numerical_data = subset_numerical_data.drop(columns=['permno'])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(subset_numerical_data)\n",
    "\n",
    "    # make a DataFrame with the scaled data\n",
    "    scaled_data = pd.DataFrame(scaled_data, columns=subset_numerical_data.columns)\n",
    "        \n",
    "    if verbose:\n",
    "        print('Successfully scaled data.')\n",
    "        \n",
    "    return scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_q(explained_variance, required_explained_var = 0.95):\n",
    "    \"\"\"\n",
    "    Finds the minimum number of principal components (q) required to explain a given amount of variance.\n",
    "\n",
    "    Parameters:\n",
    "    explained_variance (list): A list of explained variances for each principal component.\n",
    "    required_explained_var (float): The required amount of variance to be explained (default is 0.95).\n",
    "\n",
    "    Returns:\n",
    "    int: The minimum number of principal components required to explain the given amount of variance.\n",
    "    \"\"\"\n",
    "    cumulative_expl_var = np.cumsum(explained_variance)\n",
    "    for i,j in enumerate(cumulative_expl_var):\n",
    "        if j >= required_explained_var:\n",
    "            q = i+1\n",
    "            break \n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_pca(data):\n",
    "    pca = PCA()\n",
    "    pca.fit(data)\n",
    "    return [pca.explained_variance_ratio_, pca.components_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_pfa(data, principal_components, q, diff_n_features):\n",
    "    \"\"\"\n",
    "    Perform feature selection using Principal Feature Analysis (PFA).\n",
    "\n",
    "    Parameters:\n",
    "    - data: numpy array\n",
    "        The input data matrix.\n",
    "    - principal_components: numpy array\n",
    "        The principal components obtained from PCA.\n",
    "    - q: int\n",
    "        The number of principal components to consider.\n",
    "    - diff_n_features: int\n",
    "        The difference between the number of features to select and the number of principal components.\n",
    "\n",
    "    Returns:\n",
    "    - indices: list\n",
    "        The indices of the selected features.\n",
    "    - features: numpy array\n",
    "        The selected features from the input data matrix.\n",
    "    \"\"\"\n",
    "    A_q = principal_components.T[:,:q]\n",
    "    clusternumber = min([q + diff_n_features, data.shape[1]])\n",
    "        \n",
    "    kmeans = KMeans(n_clusters = clusternumber).fit(A_q)\n",
    "    clusters = kmeans.predict(A_q)\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "\n",
    "    dists = defaultdict(list)\n",
    "    for i, c in enumerate(clusters):\n",
    "        dist = euclidean_distances([A_q[i, :]], [cluster_centers[c, :]])[0][0]\n",
    "        dists[c].append((i, dist))\n",
    "\n",
    "    indices = [sorted(f, key=lambda x: x[1])[0][0] for f in dists.values()]\n",
    "    features = data[:, indices]\n",
    "    return indices, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_pca(data, fitted, principal_components, q, preprocess_data=None):\n",
    "    \"\"\"\n",
    "    Transforms the input data using Principal Component Analysis (PCA).\n",
    "\n",
    "    Args:\n",
    "        data (array-like): The input data to be transformed.\n",
    "        fitted (bool): Indicates whether the PCA model has been fitted to the data.\n",
    "        principal_components (array-like): The principal components obtained from the PCA model.\n",
    "        q (int): The number of principal components to keep in the transformed data.\n",
    "        preprocess_data (function, optional): A function to preprocess the data before transformation.\n",
    "\n",
    "    Returns:\n",
    "        array-like: The transformed data with reduced dimensions.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If the model has not been fitted to the data.\n",
    "    \"\"\"\n",
    "    if preprocess_data is not None:\n",
    "        scaled_data = preprocess_data(data)\n",
    "    else:\n",
    "        scaled_data = data\n",
    "\n",
    "    if not fitted:\n",
    "        raise Exception('The model has not been fitted to the data.')\n",
    "\n",
    "    reduced_data = np.matmul(np.array(scaled_data), np.transpose(principal_components))[:, :q]\n",
    "    return reduced_data\n",
    "\n",
    "def transform_pfa(data, fitted, features, preprocess_data=None):\n",
    "    if preprocess_data != None:\n",
    "        scaled_data = preprocess_data(data)\n",
    "    else:\n",
    "        scaled_data = data\n",
    "\n",
    "    if fitted != True:\n",
    "        raise Exception('The model has not been fitted to the data.')\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_transform(data, method):\n",
    "    \"\"\"\n",
    "    Applies feature selection to the input data using the specified method.\n",
    "\n",
    "    Args:\n",
    "        data (numpy.ndarray): The input data to be transformed.\n",
    "        method (str): The feature selection method to be used. Must be either 'pca' or 'pfa'.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The transformed data after applying feature selection.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If the method is not 'pca' or 'pfa'.\n",
    "    \"\"\"\n",
    "    if method not in ['pca', 'pfa']:\n",
    "        raise Exception(\"Method must be either 'pca' or 'pfa'\")\n",
    "    scaled_data = preprocess_data(data)\n",
    "    if method == 'PCA':\n",
    "        explained_variance, principal_components = fit_pca(scaled_data)\n",
    "        q = find_q(explained_variance)\n",
    "        output = transform_pca(scaled_data, True, principal_components, q)\n",
    "    elif method == 'PFA':\n",
    "        explained_variance, principal_components = fit_pca(scaled_data)\n",
    "        q = find_q(explained_variance)\n",
    "        diff_n_features = 0\n",
    "        indices, features = fit_pfa(scaled_data, principal_components, q, diff_n_features)\n",
    "        output = transform_pfa(scaled_data, True, features)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open data\n",
    "stationary_data = pd.read_csv('../../data/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropping dprefftaxpretret_noapretret_earnatint_debtint_totdebtinvt_actrect_actcurr_debtprofit_lctocf_lctintcovintcov_ratiocash_ratioquick_ratiocurr_ratioinv_turnnaics_processedret_industry_totret_industry_relative columns:\n",
      " Index(['dpr', 'efftax', 'pretret_noa', 'pretret_earnat', 'int_debt',\n",
      "       'int_totdebt', 'invt_act', 'rect_act', 'curr_debt', 'profit_lct',\n",
      "       'ocf_lct', 'intcov', 'intcov_ratio', 'cash_ratio', 'quick_ratio',\n",
      "       'curr_ratio', 'inv_turn', 'naics_processed', 'ret_industry_tot',\n",
      "       'ret_industry_relative'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "nan_threshold = 0.07\n",
    "\n",
    "# find the number of nan values in each column\n",
    "nan_values = stationary_data.isna().sum()\n",
    "\n",
    "# find the columns with more than 7% nan values\n",
    "columns_to_drop = nan_values[nan_values > (nan_threshold * stationary_data.shape[0])].index\n",
    "\n",
    "# drop the columns with more than 7% nan values\n",
    "print(f'dropping {np.sum(columns_to_drop)} columns:\\n', columns_to_drop)\n",
    "stationary_data = stationary_data.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column: \n",
      " date                  633\n",
      "permno                  0\n",
      "CAPEI               32101\n",
      "bm                 105895\n",
      "evm                 15009\n",
      "pe_op_basic         56727\n",
      "pe_op_dil           57333\n",
      "pe_exi              57515\n",
      "pe_inc              57251\n",
      "ps                   3187\n",
      "pcf                  6685\n",
      "npm                  3187\n",
      "opmbd                3187\n",
      "opmad                3187\n",
      "gpm                  3374\n",
      "ptpm                 3187\n",
      "cfm                 19635\n",
      "roa                  6790\n",
      "roe                112781\n",
      "roce                19492\n",
      "aftret_eq            6586\n",
      "aftret_invcapx      44834\n",
      "aftret_equity        6586\n",
      "GProf                2957\n",
      "equity_invcap        4525\n",
      "debt_invcap         14426\n",
      "totdebt_invcap      15497\n",
      "capital_ratio       12261\n",
      "cash_lt              5076\n",
      "debt_at             13332\n",
      "debt_ebitda         25981\n",
      "short_debt         181951\n",
      "lt_debt             12261\n",
      "cash_debt           26811\n",
      "fcf_ocf            159241\n",
      "lt_ppent            40062\n",
      "dltt_be            110001\n",
      "debt_assets          5076\n",
      "debt_capital        30805\n",
      "de_ratio             5076\n",
      "at_turn              7000\n",
      "rect_turn           47305\n",
      "pay_turn           109770\n",
      "sale_invcap          5312\n",
      "sale_equity        110380\n",
      "rd_sale              2232\n",
      "adv_sale             3554\n",
      "staff_sale           3554\n",
      "accrual              6663\n",
      "ptb                105895\n",
      "prc                   633\n",
      "vol                   637\n",
      "ret                   633\n",
      "retx                  633\n",
      "mktcap                633\n",
      "prc_adj               633\n",
      "MACD_index            633\n",
      "rsi                   633\n",
      "12_month_return    181930\n",
      "3_month_return      46242\n",
      "divyield             1925\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Total number of missing values:  1927331\n",
      "Number of rows with missing values:  786256\n"
     ]
    }
   ],
   "source": [
    "# set the print limit to 100\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "# print the number of missing values in each column\n",
    "print(f'Missing values per column: \\n', stationary_data.isnull().sum())\n",
    "print('\\n')\n",
    "print('Total number of missing values: ', stationary_data.isnull().sum().sum())\n",
    "\n",
    "# count the number of rows with missing values\n",
    "print('Number of rows with missing values: ', stationary_data.isnull().any(axis=1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with missing values\n",
    "stationary_data_full = stationary_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(data, start_date, period_duration, periods, explained_variance_threshold=0.95, diff_n_features=2):\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "\n",
    "    assert data.isnull().sum().sum() == 0, 'Data contains missing values.'\n",
    "\n",
    "    # create a dictionary to store the features\n",
    "    features_dict = {}\n",
    "    \n",
    "    for i in range(periods):\n",
    "        new_start_date = start_date + pd.DateOffset(years=(period_duration*i))\n",
    "        end_date = new_start_date + pd.DateOffset(years=period_duration) - pd.DateOffset(days=1)\n",
    "\n",
    "        # preprocess the data\n",
    "        scaled_data = preprocess_data(data.copy(), start_date=new_start_date, end_date=end_date, verbose=True)\n",
    "\n",
    "        # fit the pca model\n",
    "        explained_variance, principal_components = fit_pca(scaled_data)\n",
    "\n",
    "        # find the number of principal components to explain the variance threshold\n",
    "        q = find_q(explained_variance, explained_variance_threshold)\n",
    "        print(f'Number of principal components to explain {explained_variance_threshold*100}% of the variance: {q}')\n",
    "\n",
    "        # fit the pfa model\n",
    "        indices, features = fit_pfa(np.array(scaled_data), principal_components, q, diff_n_features)\n",
    "\n",
    "        # find the list of features\n",
    "        features_list = list(scaled_data.columns[indices])\n",
    "\n",
    "        # store the features in the dictionary\n",
    "        features_dict[new_start_date] = features_list\n",
    "\n",
    "        print(f'Succesfully extracted features for period starting in {new_start_date}.\\n')\n",
    "\n",
    "    return features_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully subsetted data from 2008-01-01 00:00:00 to 2009-12-31 00:00:00.\n",
      "Successfully removed columns with non-numerical values: ['date']\n",
      "Successfully scaled data.\n",
      "Number of principal components to explain 80.0% of the variance: 22\n",
      "Succesfully extracted features for period starting in 2008-01-01 00:00:00.\n",
      "\n",
      "Successfully subsetted data from 2010-01-01 00:00:00 to 2011-12-31 00:00:00.\n",
      "Successfully removed columns with non-numerical values: ['date']\n",
      "Successfully scaled data.\n",
      "Number of principal components to explain 80.0% of the variance: 22\n",
      "Succesfully extracted features for period starting in 2010-01-01 00:00:00.\n",
      "\n",
      "Successfully subsetted data from 2012-01-01 00:00:00 to 2013-12-31 00:00:00.\n",
      "Successfully removed columns with non-numerical values: ['date']\n",
      "Successfully scaled data.\n",
      "Number of principal components to explain 80.0% of the variance: 20\n",
      "Succesfully extracted features for period starting in 2012-01-01 00:00:00.\n",
      "\n",
      "Successfully subsetted data from 2014-01-01 00:00:00 to 2015-12-31 00:00:00.\n",
      "Successfully removed columns with non-numerical values: ['date']\n",
      "Successfully scaled data.\n",
      "Number of principal components to explain 80.0% of the variance: 21\n",
      "Succesfully extracted features for period starting in 2014-01-01 00:00:00.\n",
      "\n",
      "Successfully subsetted data from 2016-01-01 00:00:00 to 2017-12-31 00:00:00.\n",
      "Successfully removed columns with non-numerical values: ['date']\n",
      "Successfully scaled data.\n",
      "Number of principal components to explain 80.0% of the variance: 21\n",
      "Succesfully extracted features for period starting in 2016-01-01 00:00:00.\n",
      "\n",
      "Successfully subsetted data from 2018-01-01 00:00:00 to 2019-12-31 00:00:00.\n",
      "Successfully removed columns with non-numerical values: ['date']\n",
      "Successfully scaled data.\n",
      "Number of principal components to explain 80.0% of the variance: 21\n",
      "Succesfully extracted features for period starting in 2018-01-01 00:00:00.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_date = '2008-01-01' # start_date of the first period\n",
    "period_duration = 2 # duration of each period in years\n",
    "periods =  6 # number of periods\n",
    "explained_variance_threshold = 0.8 # threshold for explained variance\n",
    "diff_n_features = 2 # difference between the number of features to select and the number of principal components\n",
    "\n",
    "features = find_features(stationary_data_full, start_date, period_duration, periods, explained_variance_threshold, diff_n_features)\n",
    "\n",
    "# save the features as a dictionary with pickle\n",
    "with open('../../data/selected_features.pkl', 'wb') as f:\n",
    "    pickle.dump(features, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features for period starting in: 2008-01-01 00:00:00:\n",
      "['CAPEI', 'divyield', 'debt_ebitda', 'pe_op_basic', 'pe_exi', 'cash_lt', 'opmad', 'roce', 'roe', 'aftret_equity', 'GProf', 'debt_at', 'totdebt_invcap', 'short_debt', 'fcf_ocf', 'sale_invcap', 'rect_turn', 'staff_sale', 'prc', 'vol', 'retx', 'MACD_index', '12_month_return', '3_month_return']\n",
      "\n",
      "Selected features for period starting in: 2010-01-01 00:00:00:\n",
      "['CAPEI', 'equity_invcap', 'evm', 'pe_op_dil', 'pe_exi', 'rd_sale', 'pcf', 'npm', 'opmbd', 'roce', 'aftret_equity', 'GProf', 'debt_invcap', 'totdebt_invcap', 'fcf_ocf', 'dltt_be', 'de_ratio', 'rect_turn', 'pay_turn', 'prc', 'mktcap', 'retx', 'prc_adj', 'rsi']\n",
      "\n",
      "Selected features for period starting in: 2012-01-01 00:00:00:\n",
      "['CAPEI', 'bm', 'debt_ebitda', 'pe_exi', 'cash_lt', 'opmad', 'roce', 'aftret_eq', 'GProf', 'debt_at', 'debt_capital', 'fcf_ocf', 'sale_equity', 'rect_turn', 'pay_turn', 'adv_sale', 'accrual', 'prc', 'vol', 'retx', '3_month_return', '12_month_return']\n",
      "\n",
      "Selected features for period starting in: 2014-01-01 00:00:00:\n",
      "['CAPEI', 'divyield', 'debt_ebitda', 'pe_op_basic', 'pe_exi', 'ps', 'opmad', 'adv_sale', 'roa', 'aftret_invcapx', 'aftret_equity', 'debt_invcap', 'short_debt', 'fcf_ocf', 'de_ratio', 'at_turn', 'rect_turn', 'pay_turn', 'prc', 'vol', 'ret', '3_month_return', '12_month_return']\n",
      "\n",
      "Selected features for period starting in: 2016-01-01 00:00:00:\n",
      "['CAPEI', 'divyield', 'evm', 'pe_op_dil', 'pe_inc', 'ps', 'ptb', 'ptpm', 'roce', 'aftret_eq', 'at_turn', 'debt_invcap', 'cash_lt', 'lt_debt', 'fcf_ocf', 'staff_sale', 'de_ratio', 'rect_turn', 'adv_sale', 'prc_adj', 'vol', 'retx', 'rsi']\n",
      "\n",
      "Selected features for period starting in: 2018-01-01 00:00:00:\n",
      "['CAPEI', 'dltt_be', 'evm', 'pe_op_dil', 'pe_exi', 'ps', 'ptpm', 'roa', 'aftret_invcapx', 'aftret_eq', 'GProf', 'debt_at', 'debt_assets', 'staff_sale', 'fcf_ocf', 'de_ratio', 'at_turn', 'prc', 'vol', 'ret', 'MACD_index', '3_month_return', 'divyield']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load the features from the pickle file\n",
    "with open('../../data/selected_features.pkl', 'rb') as f:\n",
    "    features_dict = pickle.load(f)\n",
    "\n",
    "for key, value in features_dict.items():\n",
    "    print(f'Selected features for period starting in: {key}:')\n",
    "    print(value)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 features that are selected in all periods.\n",
      "Features that are selected in all periods:\n",
      "['CAPEI', 'fcf_ocf']\n"
     ]
    }
   ],
   "source": [
    "# find features that are in all periods\n",
    "features_in_all_periods = []\n",
    "\n",
    "for key, value in features_dict.items():\n",
    "    if len(features_in_all_periods) == 0:\n",
    "        features_in_all_periods = value\n",
    "    else:\n",
    "        features_in_all_periods = [f for f in features_in_all_periods if f in value]\n",
    "\n",
    "print(f'There are {len(features_in_all_periods)} features that are selected in all periods.')\n",
    "print('Features that are selected in all periods:')\n",
    "print(features_in_all_periods)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
