{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_stock_data(data, start_date, end_date, verbose=False):\n",
    "    \"\"\"\n",
    "    Subsets the given dataframe based on a specified date range.\n",
    "\n",
    "    Args:\n",
    "        data (pandas.DataFrame): The dataframe containing the stock data.\n",
    "        start_date (str or pandas.Timestamp): The start date of the desired date range.\n",
    "        end_date (str or pandas.Timestamp): The end date of the desired date range.\n",
    "        verbose (bool, optional): If True, prints a success message. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The subset of the dataframe based on the specified date range.\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If the dataframe does not contain a 'Date' column.\n",
    "\n",
    "    \"\"\"\n",
    "    # Check if 'Date' column exists in the dataframe\n",
    "    if 'date' not in data.columns:\n",
    "        raise ValueError(\"DataFrame does not contain a 'Date' column.\")\n",
    "     \n",
    "    # Convert date columns to datetime if they are not already datetime objects\n",
    "    if not isinstance(data['date'], pd.DatetimeIndex):\n",
    "        data['date'] = pd.to_datetime(data['date'])\n",
    "    \n",
    "    if not isinstance(start_date, pd.Timestamp):\n",
    "        start_date = pd.to_datetime(start_date)\n",
    "\n",
    "    if not isinstance(end_date, pd.Timestamp):\n",
    "        end_date = pd.to_datetime(end_date)\n",
    "\n",
    "    # Subset the dataframe based on date range\n",
    "    subset = data[(data['date'] >= start_date) & (data['date'] <=end_date)]\n",
    "    if verbose:\n",
    "        print(f'Successfully subsetted data from {start_date} to {end_date}.')\n",
    "    return subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_nans(df, interpolate=True):\n",
    "\n",
    "    print(\"Mising value data before handling from 2008 onwards: \")\n",
    "\n",
    "    # subset data from 2008 onwards\n",
    "    df = df[df['date'].dt.year >= 2008]\n",
    "\n",
    "    # set the print limit to 100\n",
    "    pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "    # find percentage of missing values per column\n",
    "    missing_values = (df.isnull().sum() / len(df)) * 100\n",
    "\n",
    "    # print the columns with missing values in descending order\n",
    "    print(f'\\nPercentage of missing values per column:\\n{missing_values.sort_values(ascending=False)[:15]}')\n",
    "\n",
    "    # print the total number of missing values\n",
    "    print('\\nTotal number of missing values: ', df.isnull().sum().sum())\n",
    "\n",
    "    # count the number of rows with missing values\n",
    "    print('Number of rows with missing values: ', df.isnull().any(axis=1).sum())\n",
    "\n",
    "    print(\"\\n\\n---------------------------------\")\n",
    "    print(\"---------------------------------\")\n",
    "    print(\"Handling missing values...\")\n",
    "\n",
    "    df_interpolated = df.copy()\n",
    "    if interpolate:         \n",
    "        \n",
    "        # find unique permnos\n",
    "        permnos = df_interpolated['permno'].unique()\n",
    "        len_permnos = len(permnos)\n",
    "\n",
    "        # interpolate missing values for each permno\n",
    "        print('')\n",
    "        for i, permno in enumerate(permnos):\n",
    "            df_interpolated.loc[df_interpolated['permno'] == permno] = df_interpolated.loc[df_interpolated['permno'] == permno].interpolate(method='linear', limit_direction='forward', limit=5)\n",
    "            print(f'Interpolating by permno ({i+1}/{len_permnos})...\\r', flush=True, end='')\n",
    "\n",
    "        print('')\n",
    "\n",
    "        # print the number of missing values after interpolation\n",
    "        print('Number of missing values after interpolation: ', df_interpolated.isnull().sum().sum(), 'instead of ', df.isnull().sum().sum())\n",
    "\n",
    "        # print the number of rows with missing values after interpolation\n",
    "        print('Number of rows with missing values after interpolation: ', df_interpolated.isnull().any(axis=1).sum(), 'instead of ', df.isnull().any(axis=1).sum())\n",
    "\n",
    "        # find percentage of missing values per column after interpolation\n",
    "        missing_values_interpolated = (df_interpolated.isnull().sum() / len(df_interpolated)) * 100\n",
    "\n",
    "        # print the columns with missing values in descending order after interpolation\n",
    "        print(f'\\nPercentage of missing values per column after interpolation:\\n{missing_values_interpolated.sort_values(ascending=False)[:15]}')\n",
    "\n",
    "    return df_interpolated    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_numerical_columns(data, verbose=False):\n",
    "    \"\"\"\n",
    "    Remove non-numerical columns from a dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    - data: pandas DataFrame\n",
    "        The input dataframe from which non-numerical columns will be removed.\n",
    "    - verbose: bool, optional\n",
    "        If True, print a message with the deleted columns. Default is False.\n",
    "\n",
    "    Returns:\n",
    "    - pandas DataFrame\n",
    "        The dataframe with non-numerical columns removed.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check first 10 rows for numerical columns\n",
    "    first_10_rows = data.head(10)\n",
    "    non_numerical_columns = []\n",
    "\n",
    "    # Iterate through columns\n",
    "    for column in data.columns:\n",
    "        # Check if the column contains numerical data\n",
    "        if pd.api.types.is_numeric_dtype(first_10_rows[column]):\n",
    "            continue\n",
    "        else:\n",
    "            non_numerical_columns.append(column)\n",
    "\n",
    "    # Remove non-numerical columns from the dataframe\n",
    "    data = data.copy()\n",
    "    data.drop(columns=non_numerical_columns, inplace=True)\n",
    "\n",
    "    # drop date column if it exists\n",
    "    if 'date' in data.columns:\n",
    "        data.drop(columns=['date'], inplace=True)\n",
    "\n",
    "    # Print message with deleted columns\n",
    "    if verbose:\n",
    "        if non_numerical_columns:\n",
    "            print(\"Successfully removed columns with non-numerical values:\", non_numerical_columns)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data, interpolate = False, start_date=None, end_date=None, verbose=False):\n",
    "    \"\"\"\n",
    "    Preprocesses the input data by performing the following steps:\n",
    "    1. Subset the data based on the specified start and end dates.\n",
    "    2. Remove non-numerical columns from the subsetted data.\n",
    "    3. Scale the numerical data using StandardScaler.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The input data to be preprocessed.\n",
    "        start_date (str, optional): The start date for subsetting the data. Defaults to None.\n",
    "        end_date (str, optional): The end date for subsetting the data. Defaults to None.\n",
    "        verbose (bool, optional): Whether to print verbose output. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The preprocessed and scaled data.\n",
    "    \"\"\"\n",
    "    if type(data) != pd.DataFrame:\n",
    "        raise Exception('data must be a pandas dataframe')\n",
    "    \n",
    "    subset_data = subset_stock_data(data, start_date, end_date, verbose=verbose)\n",
    "    subset_numerical_data = remove_non_numerical_columns(subset_data, verbose=verbose)\n",
    "\n",
    "    if interpolate:\n",
    "        subset_numerical_data = subset_numerical_data.interpolate(limit_direction='forward')\n",
    "\n",
    "    if 'permno' in subset_numerical_data.columns:\n",
    "        subset_numerical_data = subset_numerical_data.drop(columns=['permno'])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(subset_numerical_data)\n",
    "\n",
    "    # make a DataFrame with the scaled data\n",
    "    scaled_data = pd.DataFrame(scaled_data, columns=subset_numerical_data.columns)\n",
    "        \n",
    "    if verbose:\n",
    "        print('Successfully scaled data.')\n",
    "        \n",
    "    return scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_q(explained_variance, required_explained_var = 0.95):\n",
    "    \"\"\"\n",
    "    Finds the minimum number of principal components (q) required to explain a given amount of variance.\n",
    "\n",
    "    Parameters:\n",
    "    explained_variance (list): A list of explained variances for each principal component.\n",
    "    required_explained_var (float): The required amount of variance to be explained (default is 0.95).\n",
    "\n",
    "    Returns:\n",
    "    int: The minimum number of principal components required to explain the given amount of variance.\n",
    "    \"\"\"\n",
    "    cumulative_expl_var = np.cumsum(explained_variance)\n",
    "    for i,j in enumerate(cumulative_expl_var):\n",
    "        if j >= required_explained_var:\n",
    "            q = i+1\n",
    "            break \n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_pca(data):\n",
    "    pca = PCA()\n",
    "    pca.fit(data)\n",
    "    return [pca.explained_variance_ratio_, pca.components_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_pfa(data, principal_components, q, diff_n_features):\n",
    "    \"\"\"\n",
    "    Perform feature selection using Principal Feature Analysis (PFA).\n",
    "\n",
    "    Parameters:\n",
    "    - data: numpy array\n",
    "        The input data matrix.\n",
    "    - principal_components: numpy array\n",
    "        The principal components obtained from PCA.\n",
    "    - q: int\n",
    "        The number of principal components to consider.\n",
    "    - diff_n_features: int\n",
    "        The difference between the number of features to select and the number of principal components.\n",
    "\n",
    "    Returns:\n",
    "    - indices: list\n",
    "        The indices of the selected features.\n",
    "    - features: numpy array\n",
    "        The selected features from the input data matrix.\n",
    "    \"\"\"\n",
    "    A_q = principal_components.T[:,:q]\n",
    "    clusternumber = min([q + diff_n_features, data.shape[1]])\n",
    "        \n",
    "    kmeans = KMeans(n_clusters = clusternumber).fit(A_q)\n",
    "    clusters = kmeans.predict(A_q)\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "\n",
    "    dists = defaultdict(list)\n",
    "    for i, c in enumerate(clusters):\n",
    "        dist = euclidean_distances([A_q[i, :]], [cluster_centers[c, :]])[0][0]\n",
    "        dists[c].append((i, dist))\n",
    "\n",
    "    indices = [sorted(f, key=lambda x: x[1])[0][0] for f in dists.values()]\n",
    "    features = data[:, indices]\n",
    "    return indices, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_pca(data, fitted, principal_components, q, preprocess_data=None):\n",
    "    \"\"\"\n",
    "    Transforms the input data using Principal Component Analysis (PCA).\n",
    "\n",
    "    Args:\n",
    "        data (array-like): The input data to be transformed.\n",
    "        fitted (bool): Indicates whether the PCA model has been fitted to the data.\n",
    "        principal_components (array-like): The principal components obtained from the PCA model.\n",
    "        q (int): The number of principal components to keep in the transformed data.\n",
    "        preprocess_data (function, optional): A function to preprocess the data before transformation.\n",
    "\n",
    "    Returns:\n",
    "        array-like: The transformed data with reduced dimensions.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If the model has not been fitted to the data.\n",
    "    \"\"\"\n",
    "    if preprocess_data is not None:\n",
    "        scaled_data = preprocess_data(data)\n",
    "    else:\n",
    "        scaled_data = data\n",
    "\n",
    "    if not fitted:\n",
    "        raise Exception('The model has not been fitted to the data.')\n",
    "\n",
    "    reduced_data = np.matmul(np.array(scaled_data), np.transpose(principal_components))[:, :q]\n",
    "    return reduced_data\n",
    "\n",
    "def transform_pfa(data, fitted, features, preprocess_data=None):\n",
    "    if preprocess_data != None:\n",
    "        scaled_data = preprocess_data(data)\n",
    "    else:\n",
    "        scaled_data = data\n",
    "\n",
    "    if fitted != True:\n",
    "        raise Exception('The model has not been fitted to the data.')\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_transform(data, method):\n",
    "    \"\"\"\n",
    "    Applies feature selection to the input data using the specified method.\n",
    "\n",
    "    Args:\n",
    "        data (numpy.ndarray): The input data to be transformed.\n",
    "        method (str): The feature selection method to be used. Must be either 'pca' or 'pfa'.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The transformed data after applying feature selection.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If the method is not 'pca' or 'pfa'.\n",
    "    \"\"\"\n",
    "    if method not in ['pca', 'pfa']:\n",
    "        raise Exception(\"Method must be either 'pca' or 'pfa'\")\n",
    "    scaled_data = preprocess_data(data)\n",
    "    if method == 'PCA':\n",
    "        explained_variance, principal_components = fit_pca(scaled_data)\n",
    "        q = find_q(explained_variance)\n",
    "        output = transform_pca(scaled_data, True, principal_components, q)\n",
    "    elif method == 'PFA':\n",
    "        explained_variance, principal_components = fit_pca(scaled_data)\n",
    "        q = find_q(explained_variance)\n",
    "        diff_n_features = 0\n",
    "        indices, features = fit_pfa(scaled_data, principal_components, q, diff_n_features)\n",
    "        output = transform_pfa(scaled_data, True, features)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open data\n",
    "stationary_data = pd.read_csv('../../data/datasetlabel.csv')\n",
    "\n",
    "# set the 'date' column as datetime \n",
    "stationary_data['date'] = pd.to_datetime(stationary_data['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mising value data before handling from 2008 onwards: \n",
      "\n",
      "Percentage of missing values per column:\n",
      "short_debt      4.044770\n",
      "sale_equity     3.784711\n",
      "dltt_be         3.705148\n",
      "roe             3.703360\n",
      "bm              3.678882\n",
      "ptb             3.678627\n",
      "pay_turn        3.117387\n",
      "fcf_ocf         2.698840\n",
      "rect_turn       1.240356\n",
      "pe_exi          1.168967\n",
      "pe_op_dil       1.167817\n",
      "pe_inc          1.160751\n",
      "pe_op_basic     1.156238\n",
      "lt_ppent        1.004775\n",
      "debt_capital    0.814914\n",
      "dtype: float64\n",
      "\n",
      "Total number of missing values:  1026036\n",
      "Number of rows with missing values:  391624\n",
      "\n",
      "\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "Handling missing values...\n",
      "\n",
      "Interpolating by permno (715/715)...\n",
      "Number of missing values after interpolation:  1013695 instead of  1026036\n",
      "Number of rows with missing values after interpolation:  388691 instead of  391624\n",
      "\n",
      "Percentage of missing values per column after interpolation:\n",
      "short_debt      4.023698\n",
      "sale_equity     3.754614\n",
      "dltt_be         3.664409\n",
      "bm              3.653255\n",
      "ptb             3.653000\n",
      "roe             3.650361\n",
      "pay_turn        3.103339\n",
      "fcf_ocf         2.653418\n",
      "rect_turn       1.231417\n",
      "pe_exi          1.150321\n",
      "pe_op_dil       1.149172\n",
      "pe_inc          1.142957\n",
      "pe_op_basic     1.138657\n",
      "lt_ppent        0.996048\n",
      "debt_capital    0.811295\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# subset stationary data from 2008 onwards and intepolate missing values\n",
    "stationary_data = handle_nans(stationary_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropping 1 columns due to containing too many NaNs or being the target:\n",
      " ['target']\n",
      "dropping 6 key features:\n",
      " ['MACD_index', 'vol', 'ret', 'prc_adj', 'retx', 'ret_industry_relative']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['target', 'MACD_index', 'vol', 'ret', 'prc_adj', 'retx', 'ret_industry_relative'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropping \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(columns_to_drop)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns due to containing too many NaNs or being the target:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, (columns_to_drop))\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropping \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(key_features)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m key features:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, (key_features))\n\u001b[1;32m---> 13\u001b[0m stationary_data \u001b[38;5;241m=\u001b[39m \u001b[43mstationary_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcolumns_to_drop\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mkey_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:5568\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5421\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5422\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5429\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5430\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5431\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5432\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5433\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5566\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5567\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5568\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5570\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5571\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5574\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5575\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5576\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:4785\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4783\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4785\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4787\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4788\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32mc:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:4827\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4825\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4826\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4827\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4828\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4830\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4831\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['target', 'MACD_index', 'vol', 'ret', 'prc_adj', 'retx', 'ret_industry_relative'] not found in axis\""
     ]
    }
   ],
   "source": [
    "nan_threshold = 0.03705147\n",
    "\n",
    "# find the number of nan values in each column\n",
    "nan_values = stationary_data.isna().sum()\n",
    "\n",
    "# find the columns with more nan values than the threshold\n",
    "columns_to_drop = list(nan_values[nan_values > (nan_threshold * stationary_data.shape[0])].index) + ['target'] # can't include tartet in the PFA\n",
    "key_features = ['MACD_index', 'vol', 'ret', 'prc_adj', 'retx', 'ret_industry_relative'] #, 'interest_rates']\n",
    "\n",
    "# drop the columns with more nan values than the threshold\n",
    "print(f'dropping {len(columns_to_drop)} columns due to containing too many NaNs or being the target:\\n', (columns_to_drop))\n",
    "print(f'dropping {len(key_features)} key features:\\n', (key_features))\n",
    "stationary_data = stationary_data.drop(columns=(columns_to_drop + key_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of missing values per column:\n",
      "dltt_be             3.664409\n",
      "bm                  3.653255\n",
      "ptb                 3.653000\n",
      "roe                 3.650361\n",
      "pay_turn            3.103339\n",
      "fcf_ocf             2.653418\n",
      "rect_turn           1.231417\n",
      "pe_exi              1.150321\n",
      "pe_op_dil           1.149172\n",
      "pe_inc              1.142957\n",
      "pe_op_basic         1.138657\n",
      "lt_ppent            0.996048\n",
      "debt_capital        0.811295\n",
      "aftret_invcapx      0.756465\n",
      "debt_ebitda         0.687800\n",
      "cash_debt           0.602661\n",
      "roce                0.486956\n",
      "totdebt_invcap      0.403519\n",
      "CAPEI               0.399901\n",
      "evm                 0.382277\n",
      "debt_invcap         0.357799\n",
      "debt_at             0.344986\n",
      "capital_ratio       0.299393\n",
      "lt_debt             0.299308\n",
      "cfm                 0.236518\n",
      "aftret_equity       0.136692\n",
      "aftret_eq           0.136606\n",
      "roa                 0.136436\n",
      "at_turn             0.136351\n",
      "accrual             0.127795\n",
      "pcf                 0.127752\n",
      "sale_invcap         0.110979\n",
      "de_ratio            0.107914\n",
      "debt_assets         0.107914\n",
      "cash_lt             0.107659\n",
      "equity_invcap       0.097868\n",
      "naics_processed     0.082458\n",
      "ret_industry_tot    0.082458\n",
      "adv_sale            0.053936\n",
      "staff_sale          0.053085\n",
      "ptpm                0.052659\n",
      "npm                 0.052659\n",
      "opmbd               0.052489\n",
      "opmad               0.052446\n",
      "gpm                 0.052403\n",
      "ps                  0.052403\n",
      "GProf               0.052063\n",
      "rd_sale             0.035844\n",
      "stat_divyeld        0.027926\n",
      "prc                 0.000000\n",
      "mktcap              0.000000\n",
      "date                0.000000\n",
      "divyield            0.000000\n",
      "rsi                 0.000000\n",
      "permno              0.000000\n",
      "dtype: float64\n",
      "Total number of missing values:  829039\n",
      "Number of rows with missing values:  308343\n"
     ]
    }
   ],
   "source": [
    "# set the print limit to 100\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "# find percentage of missing values per column\n",
    "missing_values = (stationary_data.isnull().sum() / len(stationary_data)) * 100\n",
    "\n",
    "# print the columns with missing values in descending order\n",
    "print(f'Percentage of missing values per column:\\n{missing_values.sort_values(ascending=False)}')\n",
    "\n",
    "\n",
    "print('Total number of missing values: ', stationary_data.isnull().sum().sum())\n",
    "\n",
    "# count the number of rows with missing values\n",
    "print('Number of rows with missing values: ', stationary_data.isnull().any(axis=1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with missing values\n",
    "stationary_data_full = stationary_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows per year:\n",
      " date\n",
      "2008    131504\n",
      "2009    133722\n",
      "2010    134305\n",
      "2011    135624\n",
      "2012    134713\n",
      "2013    135198\n",
      "2014    135953\n",
      "2015    133633\n",
      "2016    128825\n",
      "2017    125501\n",
      "2018    123914\n",
      "2019    121660\n",
      "2020    120178\n",
      "2021    117101\n",
      "2022    115893\n",
      "2023    113016\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# find the number of rows per year\n",
    "rows_per_year = stationary_data_full['date'].dt.year.value_counts().sort_index()\n",
    "\n",
    "# print the number of rows per year\n",
    "print('Number of rows per year:\\n', rows_per_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(data, start_date, period_duration, periods, explained_variance_threshold=0.95, diff_n_features=2, key_features=None):\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "\n",
    "    assert data.isnull().sum().sum() == 0, 'Data contains missing values.'\n",
    "\n",
    "    # create a dictionary to store the features\n",
    "    features_dict = {}\n",
    "    \n",
    "    for i in range(periods):\n",
    "        new_start_date = start_date + pd.DateOffset(years=(period_duration*i))\n",
    "        end_date = new_start_date + pd.DateOffset(years=period_duration) - pd.DateOffset(days=1)\n",
    "\n",
    "        # preprocess the data\n",
    "        scaled_data = preprocess_data(data.copy(), start_date=new_start_date, end_date=end_date, verbose=True)\n",
    "\n",
    "        # fit the pca model\n",
    "        explained_variance, principal_components = fit_pca(scaled_data)\n",
    "\n",
    "        # find the number of principal components to explain the variance threshold\n",
    "        q = find_q(explained_variance, explained_variance_threshold)\n",
    "        print(f'Number of principal components to explain {explained_variance_threshold*100}% of the variance: {q}')\n",
    "\n",
    "        # fit the pfa model\n",
    "        indices, features = fit_pfa(np.array(scaled_data), principal_components, q, diff_n_features)\n",
    "        \n",
    "        # find the list of features\n",
    "        features_list = list(scaled_data.columns[indices])\n",
    "        if key_features != None:\n",
    "            features_list = features_list + key_features\n",
    "\n",
    "        # store the features in the dictionary\n",
    "        features_dict[new_start_date] = features_list\n",
    "\n",
    "        print(f'Succesfully extracted features for period starting in {new_start_date}.\\n')\n",
    "\n",
    "    return features_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully subsetted data from 2008-01-01 00:00:00 to 2009-12-31 00:00:00.\n",
      "Successfully removed columns with non-numerical values: ['date']\n",
      "Successfully scaled data.\n",
      "Number of principal components to explain 80.0% of the variance: 22\n",
      "Succesfully extracted features for period starting in 2008-01-01 00:00:00.\n",
      "\n",
      "Successfully subsetted data from 2010-01-01 00:00:00 to 2011-12-31 00:00:00.\n",
      "Successfully removed columns with non-numerical values: ['date']\n",
      "Successfully scaled data.\n",
      "Number of principal components to explain 80.0% of the variance: 22\n",
      "Succesfully extracted features for period starting in 2010-01-01 00:00:00.\n",
      "\n",
      "Successfully subsetted data from 2012-01-01 00:00:00 to 2013-12-31 00:00:00.\n",
      "Successfully removed columns with non-numerical values: ['date']\n",
      "Successfully scaled data.\n",
      "Number of principal components to explain 80.0% of the variance: 22\n",
      "Succesfully extracted features for period starting in 2012-01-01 00:00:00.\n",
      "\n",
      "Successfully subsetted data from 2014-01-01 00:00:00 to 2015-12-31 00:00:00.\n",
      "Successfully removed columns with non-numerical values: ['date']\n",
      "Successfully scaled data.\n",
      "Number of principal components to explain 80.0% of the variance: 22\n",
      "Succesfully extracted features for period starting in 2014-01-01 00:00:00.\n",
      "\n",
      "Successfully subsetted data from 2016-01-01 00:00:00 to 2017-12-31 00:00:00.\n",
      "Successfully removed columns with non-numerical values: ['date']\n",
      "Successfully scaled data.\n",
      "Number of principal components to explain 80.0% of the variance: 21\n",
      "Succesfully extracted features for period starting in 2016-01-01 00:00:00.\n",
      "\n",
      "Successfully subsetted data from 2018-01-01 00:00:00 to 2019-12-31 00:00:00.\n",
      "Successfully removed columns with non-numerical values: ['date']\n",
      "Successfully scaled data.\n",
      "Number of principal components to explain 80.0% of the variance: 22\n",
      "Succesfully extracted features for period starting in 2018-01-01 00:00:00.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_date = '2008-01-01' # start_date of the first period\n",
    "period_duration = 2 # duration of each period in years\n",
    "periods =  6 # number of periods\n",
    "explained_variance_threshold = 0.8 # threshold for explained variance\n",
    "diff_n_features = 2 # difference between the number of features to select and the number of principal components\n",
    "\n",
    "features = find_features(stationary_data_full, start_date, period_duration, periods, explained_variance_threshold, diff_n_features, key_features)\n",
    "\n",
    "# save the features as a dictionary with pickle\n",
    "with open('../../data/selected_features.pkl', 'wb') as f:\n",
    "    pickle.dump(features, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features for period starting in: 2008-01-01 00:00:00:\n",
      "['CAPEI', 'bm', 'debt_ebitda', 'pe_op_basic', 'pe_inc', 'ps', 'ptpm', 'roce', 'roe', 'aftret_equity', 'accrual', 'at_turn', 'debt_assets', 'debt_at', 'cash_lt', 'fcf_ocf', 'de_ratio', 'rect_turn', 'pay_turn', 'adv_sale', 'naics_processed', 'stat_divyeld', 'mktcap', 'ret_industry_tot', 'MACD_index', 'vol', 'ret', 'prc_adj', 'retx', 'ret_industry_relative']\n",
      "\n",
      "Selected features for period starting in: 2010-01-01 00:00:00:\n",
      "['CAPEI', 'debt_invcap', 'evm', 'pe_op_basic', 'pe_exi', 'ps', 'pcf', 'npm', 'opmad', 'GProf', 'aftret_invcapx', 'aftret_eq', 'equity_invcap', 'cash_lt', 'fcf_ocf', 'staff_sale', 'de_ratio', 'at_turn', 'rect_turn', 'pay_turn', 'stat_divyeld', 'prc', 'mktcap', 'ret_industry_tot', 'MACD_index', 'vol', 'ret', 'prc_adj', 'retx', 'ret_industry_relative']\n",
      "\n",
      "Selected features for period starting in: 2012-01-01 00:00:00:\n",
      "['CAPEI', 'debt_invcap', 'debt_ebitda', 'pe_op_dil', 'pe_inc', 'gpm', 'pcf', 'ptpm', 'roce', 'roe', 'aftret_equity', 'GProf', 'cash_debt', 'fcf_ocf', 'de_ratio', 'at_turn', 'rect_turn', 'pay_turn', 'adv_sale', 'staff_sale', 'divyield', 'prc', 'mktcap', 'ret_industry_tot', 'MACD_index', 'vol', 'ret', 'prc_adj', 'retx', 'ret_industry_relative']\n",
      "\n",
      "Selected features for period starting in: 2014-01-01 00:00:00:\n",
      "['CAPEI', 'opmbd', 'debt_ebitda', 'pe_op_dil', 'pe_inc', 'ps', 'ptpm', 'roce', 'dltt_be', 'aftret_eq', 'GProf', 'equity_invcap', 'debt_invcap', 'cash_lt', 'fcf_ocf', 'staff_sale', 'de_ratio', 'sale_invcap', 'rect_turn', 'adv_sale', 'divyield', 'prc', 'mktcap', 'rsi', 'MACD_index', 'vol', 'ret', 'prc_adj', 'retx', 'ret_industry_relative']\n",
      "\n",
      "Selected features for period starting in: 2016-01-01 00:00:00:\n",
      "['CAPEI', 'bm', 'evm', 'pe_op_dil', 'pe_exi', 'ps', 'ptpm', 'dltt_be', 'aftret_equity', 'at_turn', 'cash_lt', 'capital_ratio', 'fcf_ocf', 'debt_assets', 'de_ratio', 'rect_turn', 'pay_turn', 'adv_sale', 'staff_sale', 'stat_divyeld', 'prc', 'mktcap', 'rsi', 'MACD_index', 'vol', 'ret', 'prc_adj', 'retx', 'ret_industry_relative']\n",
      "\n",
      "Selected features for period starting in: 2018-01-01 00:00:00:\n",
      "['CAPEI', 'bm', 'evm', 'pe_op_dil', 'pe_exi', 'ps', 'pcf', 'ptpm', 'roce', 'aftret_eq', 'at_turn', 'cash_lt', 'debt_capital', 'debt_at', 'fcf_ocf', 'dltt_be', 'de_ratio', 'rect_turn', 'naics_processed', 'adv_sale', 'divyield', 'prc', 'mktcap', 'rsi', 'MACD_index', 'vol', 'ret', 'prc_adj', 'retx', 'ret_industry_relative']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load the features from the pickle file\n",
    "with open('../../data/selected_features.pkl', 'rb') as f:\n",
    "    features_dict = pickle.load(f)\n",
    "\n",
    "for key, value in features_dict.items():\n",
    "    print(f'Selected features for period starting in: {key}:')\n",
    "    print(value)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11 features that are selected in all periods.\n",
      "Features that are selected in all periods but not key feaures:\n",
      "['fcf_ocf', 'de_ratio', 'CAPEI', 'mktcap', 'rect_turn']\n",
      "Key features:\n",
      "['MACD_index', 'vol', 'ret', 'prc_adj', 'retx', 'ret_industry_relative']\n"
     ]
    }
   ],
   "source": [
    "# find features that are in all periods\n",
    "features_in_all_periods = []\n",
    "\n",
    "for key, value in features_dict.items():\n",
    "    if len(features_in_all_periods) == 0:\n",
    "        features_in_all_periods = value\n",
    "    else:\n",
    "        features_in_all_periods = [f for f in features_in_all_periods if f in value]\n",
    "\n",
    "print(f'There are {len(features_in_all_periods)} features that are selected in all periods.')\n",
    "print('Features that are selected in all periods but not key feaures:')\n",
    "print(list(set(features_in_all_periods) - set(key_features)))\n",
    "print('Key features:')\n",
    "print(key_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature counts:\n",
      "CAPEI: 6\n",
      "fcf_ocf: 6\n",
      "de_ratio: 6\n",
      "rect_turn: 6\n",
      "mktcap: 6\n",
      "MACD_index: 6\n",
      "vol: 6\n",
      "ret: 6\n",
      "prc_adj: 6\n",
      "retx: 6\n",
      "ret_industry_relative: 6\n",
      "ps: 5\n",
      "ptpm: 5\n",
      "at_turn: 5\n",
      "cash_lt: 5\n",
      "adv_sale: 5\n",
      "prc: 5\n",
      "roce: 4\n",
      "pay_turn: 4\n",
      "staff_sale: 4\n",
      "pe_op_dil: 4\n",
      "bm: 3\n",
      "debt_ebitda: 3\n",
      "pe_inc: 3\n",
      "aftret_equity: 3\n",
      "stat_divyeld: 3\n",
      "ret_industry_tot: 3\n",
      "debt_invcap: 3\n",
      "evm: 3\n",
      "pe_exi: 3\n",
      "pcf: 3\n",
      "GProf: 3\n",
      "aftret_eq: 3\n",
      "divyield: 3\n",
      "dltt_be: 3\n",
      "rsi: 3\n",
      "pe_op_basic: 2\n",
      "roe: 2\n",
      "debt_assets: 2\n",
      "debt_at: 2\n",
      "naics_processed: 2\n",
      "equity_invcap: 2\n",
      "accrual: 1\n",
      "npm: 1\n",
      "opmad: 1\n",
      "aftret_invcapx: 1\n",
      "gpm: 1\n",
      "cash_debt: 1\n",
      "opmbd: 1\n",
      "sale_invcap: 1\n",
      "capital_ratio: 1\n",
      "debt_capital: 1\n"
     ]
    }
   ],
   "source": [
    "# find how many times each feature is selected\n",
    "feature_counts = {}\n",
    "for key, value in features_dict.items():\n",
    "    for feature in value:\n",
    "        if feature in feature_counts:\n",
    "            feature_counts[feature] += 1\n",
    "        else:\n",
    "            feature_counts[feature] = 1\n",
    "\n",
    "# order the counts by value in descending order\n",
    "feature_counts = dict(sorted(feature_counts.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "# print the feature counts\n",
    "print('Feature counts:')\n",
    "for key, value in feature_counts.items():\n",
    "    print(f'{key}: {value}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
