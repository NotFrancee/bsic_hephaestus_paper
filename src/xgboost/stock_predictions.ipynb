{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "data = pd.read_csv('../../data/definitive_dataset.csv')\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "# load selected features with pickle\n",
    "with open('../../data/selected_features.pkl', 'rb') as f:\n",
    "    selected_features = pickle.load(f)\n",
    "\n",
    "price_data = pd.read_csv('../../data/non_diff_adjusted_price_data.csv')\n",
    "price_data['date'] = pd.to_datetime(price_data['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO NEED TO RUN THIS CELL\n",
    "\n",
    "for period in selected_features:\n",
    "    start_date = pd.to_datetime(period)\n",
    "    end_date = start_date + pd.DateOffset(years=2) - pd.DateOffset(days=91)\n",
    "\n",
    "    training_data = data[(data['date'] >= start_date) & (data['date'] <= end_date)]\n",
    "    training_features = selected_features[start_date]\n",
    "\n",
    "    X = training_data[training_features].copy()\n",
    "    y = training_data['target'].copy()\n",
    "    negative_mask = (y == -1)\n",
    "    y[negative_mask] = 0\n",
    "\n",
    "    # import the model with pickle\n",
    "    n_estimators = 500\n",
    "    start_date_str = start_date.strftime('%Y-%m-%d')\n",
    "    model_name = f'xg_boost_{start_date_str}_{n_estimators}_correct_dataset.pkl'\n",
    "    try:\n",
    "        with open(f'../../models/final_models/{model_name}', 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f'Model {model_name} not found')\n",
    "        print('')\n",
    "        print('-----------------------------------------------------------------------------------------------------------------')    \n",
    "        continue\n",
    "    first_trading_date = start_date + pd.DateOffset(years=2)\n",
    "    dates = data['date'].unique()\n",
    "\n",
    "    # Print basic infomation:\n",
    "    print(f'Period: {period}\\n')\n",
    "    print(f'Training features:\\n{training_features}')\n",
    "\n",
    "    # find the first trading day after the end date\n",
    "    while first_trading_date not in dates:\n",
    "        first_trading_date += pd.DateOffset(days=1)\n",
    "\n",
    "    print(f'First trading date: {first_trading_date}')\n",
    "\n",
    "    X_prediction_mask = data['date'] == first_trading_date\n",
    "    X_prediction = data[X_prediction_mask][training_features].copy()\n",
    "\n",
    "    assert len(X_prediction) > 0, 'No data to predict'\n",
    "    permnos_first_trading_day = data[X_prediction_mask]['permno'].copy()\n",
    "    target = data[X_prediction_mask]['target'].copy()\n",
    "    target[target == -1] = 0\n",
    "\n",
    "\n",
    "    return_date = first_trading_date + pd.DateOffset(days=90)\n",
    "\n",
    "    # find the first trading day after the end date\n",
    "    while return_date not in dates:\n",
    "        return_date -= pd.DateOffset(days=1)\n",
    "\n",
    "    print(f'Return date: {return_date}')\n",
    "\n",
    "    X_return_mask = price_data['date'] == return_date\n",
    "    X_prediction_mask = price_data['date'] == first_trading_date\n",
    "\n",
    "    # calculate 90 days return\n",
    "    trading_day_price = price_data[X_prediction_mask][['permno', 'prc_adj']].copy()\n",
    "    return_day_price = price_data[X_return_mask]['prc_adj'].copy()\n",
    "    permnos_return = price_data[X_return_mask]['permno'].copy()\n",
    "\n",
    "\n",
    "    # find the predictions\n",
    "    y_pred_proba = model.predict_proba(X_prediction)\n",
    "    y_pred = model.predict(X_prediction)\n",
    "\n",
    "    # check if prediction is correct\n",
    "    correct_prediction = y_pred == target\n",
    "\n",
    "    # put the predictions in a dataframe\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'permno': permnos_first_trading_day,\n",
    "        '0_probability': y_pred_proba[:, 0],\n",
    "        '1_probability': y_pred_proba[:, 1],\n",
    "        'prediction': y_pred,\n",
    "        'correct_prediction': correct_prediction\n",
    "    })\n",
    "    trading_day_price.rename(columns={'prc_adj': 'trading_day_price'}, inplace=True)\n",
    "    predictions_df = pd.merge(predictions_df, trading_day_price, left_on='permno', right_on='permno', how='inner')\n",
    "\n",
    "    return_price_df = pd.DataFrame({\n",
    "        'permnos': permnos_return,\n",
    "        'price_return': return_day_price,\n",
    "    })\n",
    "\n",
    "    # merge the two dataframes\n",
    "    merged_df = pd.merge(predictions_df, return_price_df, left_on='permno', right_on='permnos')\n",
    "\n",
    "    # calculate the return\n",
    "    merged_df['return(%)'] = (merged_df['price_return'] / merged_df['trading_day_price'] - 1)*100\n",
    "\n",
    "\n",
    "    # Sanity check to see if everything lines up \n",
    "    merged_df_2 = pd.merge(merged_df, price_data[price_data['date'] == first_trading_date][['permno', 'target']], left_on='permno', right_on='permno')\n",
    "    display(merged_df_2)\n",
    "\n",
    "\n",
    "\n",
    "    n = 15\n",
    "    # find the n argmin and argmax\n",
    "    top_n = merged_df['1_probability'].nlargest(n)\n",
    "    bottom_n = merged_df['1_probability'].nsmallest(n)\n",
    "\n",
    "    #features_to_print = ['permno','0_probability', '1_probability', 'correct_prediction', 'return(%)']\n",
    "\n",
    "    print(f'Top {n} long predictions:')\n",
    "    display(merged_df.loc[top_n.index])#[features_to_print])\n",
    "    print('-'*58)\n",
    "    print(f'Top {n} short predictions:')\n",
    "    display(merged_df.loc[bottom_n.index])#[features_to_print])\n",
    "\n",
    "\n",
    "    # gain is usually the most realiable metric to evaluate the importance of the features\n",
    "    xgb.plot_importance(model, importance_type='gain')\n",
    "    plt.show()\n",
    "\n",
    "    print('')\n",
    "    print('-----------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to predict the future stock direction, for each model it makes predictions every 3 months for the next 2 years. \n",
    "# It should then return the top n stock it has the most confidence in both long and short direction for each period\n",
    "\n",
    "def get_predictions(period, n_estimators=500, short=True, number_of_stocks=15):\n",
    "    start_date = pd.to_datetime(period)\n",
    "    end_date = start_date + pd.DateOffset(years=2) - pd.DateOffset(days=91)\n",
    "\n",
    "    training_data = data[(data['date'] >= start_date) & (data['date'] <= end_date)]\n",
    "    training_features = selected_features[start_date]\n",
    "    \n",
    "    X = training_data[training_features].copy()\n",
    "    y = training_data['target'].copy()\n",
    "    negative_mask = (y == -1)\n",
    "    y[negative_mask] = 0\n",
    "\n",
    "    # import the model with pickle\n",
    "    start_date_str = start_date.strftime('%Y-%m-%d')\n",
    "    model_name = f'xg_boost_{start_date_str}_{n_estimators}_correct_dataset.pkl'\n",
    "    try:\n",
    "        with open(f'../../models/final_models/{model_name}', 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f'Model {model_name} not found')\n",
    "        print('')\n",
    "        if short:\n",
    "            return None, None\n",
    "        return None\n",
    "\n",
    "    first_trading_date = start_date + pd.DateOffset(years=2)\n",
    "    dates = data['date'].unique()\n",
    "    # find the first trading day after the end date\n",
    "    while first_trading_date not in dates:\n",
    "        first_trading_date += pd.DateOffset(days=1)\n",
    "    print(first_trading_date)\n",
    "    dates = data['date'].unique()\n",
    "\n",
    "    output_dict = {}\n",
    "    if short:\n",
    "        output_dict_short = {}\n",
    "    for i in range(8):\n",
    "        X_prediction_mask = (data['date'] == first_trading_date)\n",
    "        X_prediction = data[X_prediction_mask][training_features].copy()\n",
    "\n",
    "        assert len(X_prediction) > 0, 'No data to predict'\n",
    "        permnos_first_trading_day = data[X_prediction_mask]['permno'].copy()\n",
    "        target = data[X_prediction_mask]['target'].copy()\n",
    "        target[target == -1] = 0\n",
    "\n",
    "        return_date = first_trading_date + pd.DateOffset(days=90)\n",
    "\n",
    "        # find the first trading day after the end date\n",
    "        while return_date not in dates:\n",
    "            if i % 2 == 1:\n",
    "                return_date += pd.DateOffset(days=1)\n",
    "            else:\n",
    "                return_date -= pd.DateOffset(days=1)\n",
    "\n",
    "        # find the predictions\n",
    "        y_pred_proba = model.predict_proba(X_prediction)\n",
    "\n",
    "        predictions_df = pd.DataFrame({\n",
    "            'permno': permnos_first_trading_day,\n",
    "            '0_probability': y_pred_proba[:, 0],\n",
    "            '1_probability': y_pred_proba[:, 1],\n",
    "        })\n",
    "\n",
    "        # find the n argmin and argmax\n",
    "        top_n = predictions_df['1_probability'].nlargest(number_of_stocks)\n",
    "        # if any element in top_n has predictions_df['1_probability'] <0.5, then we remove it\n",
    "        top_n = top_n[predictions_df.loc[top_n.index]['1_probability'] >= 0.5]\n",
    "        if short:\n",
    "            bottom_n = predictions_df['1_probability'].nsmallest(number_of_stocks)\n",
    "            # if any element in bottom_n has predictions_df['1_probability'] >0.5, then we remove it\n",
    "            bottom_n = bottom_n[predictions_df.loc[bottom_n.index]['1_probability'] <= 0.5]\n",
    "            \n",
    "        # put the predictions in output_dict\n",
    "        output_dict[f'{start_date}_{i}'] = {\n",
    "            'permno': predictions_df.loc[top_n.index]['permno'].values,\n",
    "            'permno_probabilities': top_n.values\n",
    "        }\n",
    "        if short:\n",
    "            output_dict_short[f'{start_date}_{i}'] = {\n",
    "                'permno': predictions_df.loc[bottom_n.index]['permno'].values,\n",
    "                'permno_probabilities': bottom_n.values\n",
    "            }\n",
    "        \n",
    "        # move to next period:\n",
    "        first_trading_date = return_date\n",
    "    if short:\n",
    "        return output_dict, output_dict_short\n",
    "    else:\n",
    "        return output_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010-01-04 00:00:00\n",
      "2012-01-03 00:00:00\n",
      "2014-01-02 00:00:00\n",
      "Model xg_boost_2014-01-01_500_correct_dataset.pkl not found\n",
      "\n",
      "Model xg_boost_2016-01-01_500_correct_dataset.pkl not found\n",
      "\n",
      "2020-01-02 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# run get_prediction for all periods\n",
    "\n",
    "all_predictions = {}\n",
    "all_short_predictions = {}\n",
    "for period in selected_features:\n",
    "    predictions, short_predictions = get_predictions(period, n_estimators=500, short=True, number_of_stocks=10)\n",
    "    if predictions is not None:\n",
    "        all_predictions.update(predictions)\n",
    "        all_short_predictions.update(short_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2008-01-01 00:00:00_0\n",
      "[89509]\n",
      "2008-01-01 00:00:00_1\n",
      "[78877]\n",
      "2008-01-01 00:00:00_2\n",
      "[90720]\n",
      "2008-01-01 00:00:00_3\n",
      "[66384]\n",
      "2008-01-01 00:00:00_4\n",
      "[91233]\n",
      "2008-01-01 00:00:00_5\n",
      "[87070]\n",
      "2008-01-01 00:00:00_6\n",
      "[87070]\n",
      "2008-01-01 00:00:00_7\n",
      "[87070]\n",
      "2010-01-01 00:00:00_0\n",
      "[82196]\n",
      "2010-01-01 00:00:00_1\n",
      "[91937]\n",
      "2010-01-01 00:00:00_2\n",
      "[57665]\n",
      "2010-01-01 00:00:00_3\n",
      "[80303]\n",
      "2010-01-01 00:00:00_4\n",
      "[89301]\n",
      "2010-01-01 00:00:00_5\n",
      "[91391]\n",
      "2010-01-01 00:00:00_6\n",
      "[90547]\n",
      "2010-01-01 00:00:00_7\n",
      "[90547]\n",
      "2012-01-01 00:00:00_0\n",
      "[76185]\n",
      "2012-01-01 00:00:00_1\n",
      "[86288]\n",
      "2012-01-01 00:00:00_2\n",
      "[66325]\n",
      "2012-01-01 00:00:00_3\n",
      "[90720]\n",
      "2012-01-01 00:00:00_4\n",
      "[27422]\n",
      "2012-01-01 00:00:00_5\n",
      "[90162]\n",
      "2012-01-01 00:00:00_6\n",
      "[92772]\n",
      "2012-01-01 00:00:00_7\n",
      "[11786]\n",
      "2018-01-01 00:00:00_0\n",
      "[16816]\n",
      "2018-01-01 00:00:00_1\n",
      "[36397]\n",
      "2018-01-01 00:00:00_2\n",
      "[36397]\n",
      "2018-01-01 00:00:00_3\n",
      "[36397]\n",
      "2018-01-01 00:00:00_4\n",
      "[49373]\n",
      "2018-01-01 00:00:00_5\n",
      "[90162]\n",
      "2018-01-01 00:00:00_6\n",
      "[90162]\n",
      "2018-01-01 00:00:00_7\n",
      "[90162]\n"
     ]
    }
   ],
   "source": [
    "for i in all_predictions:\n",
    "    print(i)\n",
    "    print(all_predictions[i]['permno'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2008-01-01 00:00:00_0\n",
      "[79588]\n",
      "[0.00274739]\n",
      "2008-01-01 00:00:00_1\n",
      "[54181]\n",
      "[0.00209514]\n",
      "2008-01-01 00:00:00_2\n",
      "[86964]\n",
      "[0.01728031]\n",
      "2008-01-01 00:00:00_3\n",
      "[65875]\n",
      "[0.00069632]\n",
      "2008-01-01 00:00:00_4\n",
      "[49656]\n",
      "[0.00061968]\n",
      "2008-01-01 00:00:00_5\n",
      "[90199]\n",
      "[0.00076272]\n",
      "2008-01-01 00:00:00_6\n",
      "[86783]\n",
      "[0.00143962]\n",
      "2008-01-01 00:00:00_7\n",
      "[83111]\n",
      "[2.3427709e-05]\n",
      "2010-01-01 00:00:00_0\n",
      "[77481]\n",
      "[0.00236557]\n",
      "2010-01-01 00:00:00_1\n",
      "[90215]\n",
      "[0.0014678]\n",
      "2010-01-01 00:00:00_2\n",
      "[90215]\n",
      "[0.00036638]\n",
      "2010-01-01 00:00:00_3\n",
      "[76614]\n",
      "[0.00055705]\n",
      "2010-01-01 00:00:00_4\n",
      "[90215]\n",
      "[0.00135231]\n",
      "2010-01-01 00:00:00_5\n",
      "[18729]\n",
      "[0.00070091]\n",
      "2010-01-01 00:00:00_6\n",
      "[58246]\n",
      "[0.00113966]\n",
      "2010-01-01 00:00:00_7\n",
      "[79906]\n",
      "[9.1062706e-05]\n",
      "2012-01-01 00:00:00_0\n",
      "[32707]\n",
      "[0.00192199]\n",
      "2012-01-01 00:00:00_1\n",
      "[67467]\n",
      "[0.00104772]\n",
      "2012-01-01 00:00:00_2\n",
      "[14295]\n",
      "[0.00196575]\n",
      "2012-01-01 00:00:00_3\n",
      "[85035]\n",
      "[0.00191758]\n",
      "2012-01-01 00:00:00_4\n",
      "[85035]\n",
      "[0.00047514]\n",
      "2012-01-01 00:00:00_5\n",
      "[38156]\n",
      "[0.00036869]\n",
      "2012-01-01 00:00:00_6\n",
      "[38156]\n",
      "[0.00047293]\n",
      "2012-01-01 00:00:00_7\n",
      "[37161]\n",
      "[0.00978831]\n",
      "2018-01-01 00:00:00_0\n",
      "[13141]\n",
      "[0.00029795]\n",
      "2018-01-01 00:00:00_1\n",
      "[90664]\n",
      "[0.0005265]\n",
      "2018-01-01 00:00:00_2\n",
      "[90664]\n",
      "[0.00337108]\n",
      "2018-01-01 00:00:00_3\n",
      "[13447]\n",
      "[0.00157733]\n",
      "2018-01-01 00:00:00_4\n",
      "[13447]\n",
      "[0.00048159]\n",
      "2018-01-01 00:00:00_5\n",
      "[49015]\n",
      "[0.00034951]\n",
      "2018-01-01 00:00:00_6\n",
      "[93436]\n",
      "[0.00010222]\n",
      "2018-01-01 00:00:00_7\n",
      "[11786]\n",
      "[0.00230145]\n"
     ]
    }
   ],
   "source": [
    "for i in all_short_predictions:\n",
    "    print(i)\n",
    "    print(all_short_predictions[i]['permno'])\n",
    "    print(all_short_predictions[i]['permno_probabilities'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
