{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_stock_data(data, start_date, end_date, verbose=False):\n",
    "    \"\"\"\n",
    "    Subsets the given dataframe based on a specified date range.\n",
    "\n",
    "    Args:\n",
    "        data (pandas.DataFrame): The dataframe containing the stock data.\n",
    "        start_date (str or pandas.Timestamp): The start date of the desired date range.\n",
    "        end_date (str or pandas.Timestamp): The end date of the desired date range.\n",
    "        verbose (bool, optional): If True, prints a success message. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The subset of the dataframe based on the specified date range.\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If the dataframe does not contain a 'Date' column.\n",
    "\n",
    "    \"\"\"\n",
    "    # Check if 'Date' column exists in the dataframe\n",
    "    if 'date' not in data.columns:\n",
    "        raise ValueError(\"DataFrame does not contain a 'Date' column.\")\n",
    "     \n",
    "    # Convert date columns to datetime if they are not already datetime objects\n",
    "    if not isinstance(data['date'], pd.DatetimeIndex):\n",
    "        data['date'] = pd.to_datetime(data['date'])\n",
    "    \n",
    "    if not isinstance(start_date, pd.Timestamp):\n",
    "        start_date = pd.to_datetime(start_date)\n",
    "\n",
    "    if not isinstance(end_date, pd.Timestamp):\n",
    "        end_date = pd.to_datetime(end_date)\n",
    "\n",
    "    # Subset the dataframe based on date range\n",
    "    subset = data[(data['date'] >= start_date) & (data['date'] <=end_date)]\n",
    "    if verbose:\n",
    "        print(f'Successfully subsetted data from {start_date} to {end_date}.')\n",
    "    return subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_numerical_columns(data, verbose=False):\n",
    "    \"\"\"\n",
    "    Remove non-numerical columns from a dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    - data: pandas DataFrame\n",
    "        The input dataframe from which non-numerical columns will be removed.\n",
    "    - verbose: bool, optional\n",
    "        If True, print a message with the deleted columns. Default is False.\n",
    "\n",
    "    Returns:\n",
    "    - pandas DataFrame\n",
    "        The dataframe with non-numerical columns removed.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check first 10 rows for numerical columns\n",
    "    first_10_rows = data.head(10)\n",
    "    non_numerical_columns = []\n",
    "\n",
    "    # Iterate through columns\n",
    "    for column in data.columns:\n",
    "        # Check if the column contains numerical data\n",
    "        if pd.api.types.is_numeric_dtype(first_10_rows[column]):\n",
    "            continue\n",
    "        else:\n",
    "            non_numerical_columns.append(column)\n",
    "\n",
    "    # Remove non-numerical columns from the dataframe\n",
    "    data = data.copy()\n",
    "    data.drop(columns=non_numerical_columns, inplace=True)\n",
    "\n",
    "    #Â drop date column if it exists\n",
    "    if 'date' in data.columns:\n",
    "        data.drop(columns=['date'], inplace=True)\n",
    "\n",
    "    # Print message with deleted columns\n",
    "    if verbose:\n",
    "        if non_numerical_columns:\n",
    "            print(\"Successfully removed columns with non-numerical values:\", non_numerical_columns)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data, interpolate = False, start_date=None, end_date=None, verbose=False):\n",
    "    \"\"\"\n",
    "    Preprocesses the input data by performing the following steps:\n",
    "    1. Subset the data based on the specified start and end dates.\n",
    "    2. Remove non-numerical columns from the subsetted data.\n",
    "    3. Scale the numerical data using StandardScaler.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The input data to be preprocessed.\n",
    "        start_date (str, optional): The start date for subsetting the data. Defaults to None.\n",
    "        end_date (str, optional): The end date for subsetting the data. Defaults to None.\n",
    "        verbose (bool, optional): Whether to print verbose output. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The preprocessed and scaled data.\n",
    "    \"\"\"\n",
    "    if type(data) != pd.DataFrame:\n",
    "        raise Exception('data must be a pandas dataframe')\n",
    "    \n",
    "    subset_data = subset_stock_data(data, start_date, end_date, verbose=verbose)\n",
    "    subset_numerical_data = remove_non_numerical_columns(subset_data, verbose=verbose)\n",
    "\n",
    "    if interpolate:\n",
    "        subset_numerical_data = subset_numerical_data.interpolate(limit_direction='forward')\n",
    "\n",
    "    subset_numerical_data = subset_numerical_data.drop(columns=['permno'])\n",
    "        \n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(subset_numerical_data)\n",
    "        \n",
    "    if verbose:\n",
    "        print('Successfully scaled data.')\n",
    "        \n",
    "    return scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_q(explained_variance, required_explained_var = 0.95):\n",
    "    \"\"\"\n",
    "    Finds the minimum number of principal components (q) required to explain a given amount of variance.\n",
    "\n",
    "    Parameters:\n",
    "    explained_variance (list): A list of explained variances for each principal component.\n",
    "    required_explained_var (float): The required amount of variance to be explained (default is 0.95).\n",
    "\n",
    "    Returns:\n",
    "    int: The minimum number of principal components required to explain the given amount of variance.\n",
    "    \"\"\"\n",
    "    cumulative_expl_var = np.cumsum(explained_variance)\n",
    "    for i,j in enumerate(cumulative_expl_var):\n",
    "        if j >= required_explained_var:\n",
    "            q = i+1\n",
    "            break \n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_pca(data):\n",
    "    pca = PCA()\n",
    "    pca.fit(data)\n",
    "    return [pca.explained_variance_ratio_, pca.components_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_pfa(data, principal_components, q, diff_n_features):\n",
    "    \"\"\"\n",
    "    Perform feature selection using Principal Feature Analysis (PFA).\n",
    "\n",
    "    Parameters:\n",
    "    - data: numpy array\n",
    "        The input data matrix.\n",
    "    - principal_components: numpy array\n",
    "        The principal components obtained from PCA.\n",
    "    - q: int\n",
    "        The number of principal components to consider.\n",
    "    - diff_n_features: int\n",
    "        The difference between the number of features to select and the number of principal components.\n",
    "\n",
    "    Returns:\n",
    "    - indices: list\n",
    "        The indices of the selected features.\n",
    "    - features: numpy array\n",
    "        The selected features from the input data matrix.\n",
    "    \"\"\"\n",
    "    A_q = principal_components.T[:,:q]\n",
    "    clusternumber = min([q + diff_n_features, data.shape[1]])\n",
    "        \n",
    "    kmeans = KMeans(n_clusters = clusternumber).fit(A_q)\n",
    "    clusters = kmeans.predict(A_q)\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "\n",
    "    dists = defaultdict(list)\n",
    "    for i, c in enumerate(clusters):\n",
    "        dist = euclidean_distances([A_q[i, :]], [cluster_centers[c, :]])[0][0]\n",
    "        dists[c].append((i, dist))\n",
    "\n",
    "    indices = [sorted(f, key=lambda x: x[1])[0][0] for f in dists.values()]\n",
    "    features = data[:, indices]\n",
    "    return indices, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_pca(data, fitted, principal_components, q, preprocess_data=None):\n",
    "    \"\"\"\n",
    "    Transforms the input data using Principal Component Analysis (PCA).\n",
    "\n",
    "    Args:\n",
    "        data (array-like): The input data to be transformed.\n",
    "        fitted (bool): Indicates whether the PCA model has been fitted to the data.\n",
    "        principal_components (array-like): The principal components obtained from the PCA model.\n",
    "        q (int): The number of principal components to keep in the transformed data.\n",
    "        preprocess_data (function, optional): A function to preprocess the data before transformation.\n",
    "\n",
    "    Returns:\n",
    "        array-like: The transformed data with reduced dimensions.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If the model has not been fitted to the data.\n",
    "    \"\"\"\n",
    "    if preprocess_data is not None:\n",
    "        scaled_data = preprocess_data(data)\n",
    "    else:\n",
    "        scaled_data = data\n",
    "\n",
    "    if not fitted:\n",
    "        raise Exception('The model has not been fitted to the data.')\n",
    "\n",
    "    print('shape of scaled data: ', np.shape(scaled_data))\n",
    "    print('shape of transpose of principal components: ', np.shape(np.transpose(principal_components)))\n",
    "    reduced_data = np.matmul(np.array(scaled_data), np.transpose(principal_components))[:, :q]\n",
    "    print('shape of reduced data: ', np.shape(reduced_data))\n",
    "    return reduced_data\n",
    "\n",
    "def transform_pfa(data, fitted, features, preprocess_data=None):\n",
    "    if preprocess_data != None:\n",
    "        scaled_data = preprocess_data(data)\n",
    "    else:\n",
    "        scaled_data = data\n",
    "\n",
    "    if fitted != True:\n",
    "        raise Exception('The model has not been fitted to the data.')\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_transform(data, method):\n",
    "    \"\"\"\n",
    "    Applies feature selection to the input data using the specified method.\n",
    "\n",
    "    Args:\n",
    "        data (numpy.ndarray): The input data to be transformed.\n",
    "        method (str): The feature selection method to be used. Must be either 'pca' or 'pfa'.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The transformed data after applying feature selection.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If the method is not 'pca' or 'pfa'.\n",
    "    \"\"\"\n",
    "    if method not in ['pca', 'pfa']:\n",
    "        raise Exception(\"Method must be either 'pca' or 'pfa'\")\n",
    "    scaled_data = preprocess_data(data)\n",
    "    if method == 'PCA':\n",
    "        explained_variance, principal_components = fit_pca(scaled_data)\n",
    "        q = find_q(explained_variance)\n",
    "        output = transform_pca(scaled_data, True, principal_components, q)\n",
    "    elif method == 'PFA':\n",
    "        explained_variance, principal_components = fit_pca(scaled_data)\n",
    "        q = find_q(explained_variance)\n",
    "        diff_n_features = 0\n",
    "        indices, features = fit_pfa(scaled_data, principal_components, q, diff_n_features)\n",
    "        output = transform_pfa(scaled_data, True, features)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open data\n",
    "stationary_data = pd.read_csv('../../data/DATA_FINAL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column: \n",
      " permno                        0\n",
      "CAPEI                     36141\n",
      "bm                       111587\n",
      "evm                       19125\n",
      "pe_op_basic               58469\n",
      "pe_op_dil                 59083\n",
      "pe_exi                    58760\n",
      "pe_inc                    58553\n",
      "ps                         9480\n",
      "pcf                        9290\n",
      "npm                        9615\n",
      "opmbd                      9921\n",
      "opmad                     10290\n",
      "gpm                        9787\n",
      "ptpm                       9781\n",
      "cfm                       25919\n",
      "roa                       13615\n",
      "roe                      118917\n",
      "roce                      26542\n",
      "aftret_eq                 12097\n",
      "aftret_invcapx            50333\n",
      "aftret_equity             12202\n",
      "GProf                      9123\n",
      "equity_invcap             11181\n",
      "debt_invcap               21315\n",
      "totdebt_invcap            22118\n",
      "capital_ratio             19169\n",
      "cash_lt                   10841\n",
      "debt_at                   20444\n",
      "debt_ebitda               31042\n",
      "short_debt               186116\n",
      "lt_debt                   19106\n",
      "cash_debt                 32768\n",
      "fcf_ocf                  163101\n",
      "lt_ppent                  47270\n",
      "dltt_be                  116401\n",
      "debt_assets               12044\n",
      "debt_capital              37771\n",
      "de_ratio                  10850\n",
      "at_turn                   14048\n",
      "rect_turn                 52363\n",
      "pay_turn                 114417\n",
      "sale_invcap               11875\n",
      "sale_equity              116407\n",
      "rd_sale                    5206\n",
      "adv_sale                   7229\n",
      "staff_sale                 4926\n",
      "accrual                   10658\n",
      "ptb                      110918\n",
      "divyield                   3189\n",
      "date                          0\n",
      "prc                        6611\n",
      "vol                           4\n",
      "ret                           4\n",
      "retx                          4\n",
      "mktcap                     6984\n",
      "prc_adj                    3501\n",
      "naics_processed          601130\n",
      "ret_industry_tot         601146\n",
      "ret_industry_relative    601134\n",
      "MACD_index                   99\n",
      "rsi                          50\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Total number of missing values:  3772070\n",
      "Number of rows with missing values:  1107972\n"
     ]
    }
   ],
   "source": [
    "#Â set the print limit to 100\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "# print the number of missing values in each column\n",
    "print(f'Missing values per column: \\n', stationary_data.isnull().sum())\n",
    "print('\\n')\n",
    "print('Total number of missing values: ', stationary_data.isnull().sum().sum())\n",
    "\n",
    "#Â count the number of rows with missing values\n",
    "print('Number of rows with missing values: ', stationary_data.isnull().any(axis=1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â drop rows with missing values\n",
    "stationary_data_full = stationary_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully subsetted data from 2008-01-01 00:00:00 to 2009-12-31 00:00:00.\n",
      "Successfully removed columns with non-numerical values: ['date']\n",
      "Successfully scaled data.\n"
     ]
    }
   ],
   "source": [
    "#Â define a start date and an end date\n",
    "start_date = '2008-01-01'\n",
    "start_date = pd.to_datetime(start_date)\n",
    "number_of_days = 365 * 2\n",
    "end_date = pd.to_datetime(start_date) + pd.DateOffset(days=number_of_days)\n",
    "\n",
    "#Â preprocess the data\n",
    "scaled_data = preprocess_data(stationary_data_full.copy(), interpolate=False, start_date=start_date, end_date=end_date, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subset_numerical shape:  (248498, 60)\n"
     ]
    }
   ],
   "source": [
    "print('subset_numerical shape: ', scaled_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q:  25\n",
      "shape of transformed data:  (248498, 27)\n"
     ]
    }
   ],
   "source": [
    "#Â perform PFA\n",
    "explained_variance, principal_components = fit_pca(scaled_data)\n",
    "\n",
    "q = find_q(explained_variance, required_explained_var=0.8)\n",
    "print('q: ', q)\n",
    "diff_n_features = 2\n",
    "indices, features = fit_pfa(scaled_data, principal_components, q, diff_n_features)\n",
    "\n",
    "#Â transform the data\n",
    "transformed_data = transform_pfa(scaled_data, True, features)\n",
    "\n",
    "print('shape of transformed data: ', np.shape(transformed_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features:\n",
      "CAPEI\n",
      "totdebt_invcap\n",
      "debt_ebitda\n",
      "pe_op_basic\n",
      "pe_inc\n",
      "ps\n",
      "ptpm\n",
      "roce\n",
      "sale_equity\n",
      "aftret_equity\n",
      "aftret_invcapx\n",
      "at_turn\n",
      "cash_lt\n",
      "debt_at\n",
      "short_debt\n",
      "fcf_ocf\n",
      "rect_turn\n",
      "pay_turn\n",
      "adv_sale\n",
      "naics_processed\n",
      "prc\n",
      "vol\n",
      "retx\n",
      "mktcap\n",
      "prc_adj\n",
      "ret_industry_tot\n",
      "ret_industry_relative\n"
     ]
    }
   ],
   "source": [
    "# print the selected features\n",
    "columns = [col for col in stationary_data.columns if col not in ['date', 'permno']]\n",
    "print('Selected features:')\n",
    "for i in indices:\n",
    "    print(columns[i])\n",
    "    \n",
    "#Â save the transformed data\n",
    "np.savetxt('../../data/pfa_transformed_data.csv', transformed_data, delimiter=',')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
